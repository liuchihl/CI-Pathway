{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c699717b-0e06-4424-ba34-9daa1fa3a41d",
   "metadata": {},
   "source": [
    "### Train a Simple Classifier with Distributed Training\n",
    "\n",
    "In this assignment, we will train a Convolutional Neural Network (CNN) on the CIFAR-10 dataset using distributed training with Hugging Face Accelerate on at least 2 GPUs. We will calculate average loss and accuracy across GPUs using `gather_for_metrics` and log the training curves to Weights & Biases (WandB).\n",
    "\n",
    "This notebook modifies the original code to incorporate distributed training and WandB logging while retaining the core CNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29007a-db78-4200-8fe5-2d9e53e561d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running on a new environment)\n",
    "# !pip install torch torchvision accelerate wandb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from accelerate import Accelerator\n",
    "import wandb\n",
    "\n",
    "# Initialize the Accelerator for distributed training\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Initialize WandB (only on the main process to avoid duplicate logging)\n",
    "if accelerator.is_main_process:\n",
    "    wandb.init(project='cifar10-distributed-training', config={\n",
    "        'architecture': 'SimpleCNN3',\n",
    "        'dataset': 'CIFAR-10',\n",
    "        'epochs': 10,\n",
    "        'batch_size': 128,\n",
    "        'learning_rate': 0.001\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04642c9f-093c-4131-8c7b-3c98d76b36d1",
   "metadata": {},
   "source": [
    "### Load and Preprocess the Data\n",
    "\n",
    "We load the CIFAR-10 dataset with normalization and create data loaders for distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036a7069-c5ca-4056-b676-dd95486fb77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms with normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR-10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "# Prepare data loaders for distributed training\n",
    "trainloader, testloader = accelerator.prepare(trainloader, testloader)\n",
    "\n",
    "# Visualize a sample image (only on main process)\n",
    "if accelerator.is_main_process:\n",
    "    sample, label = next(iter(trainset))\n",
    "    plt.imshow(np.transpose(np.array(sample), (1, 2, 0)) * 0.5 + 0.5)  # Denormalize for display\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f'Label: {label}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aedd0d-ad5b-4ca2-af8e-388e9ba1c757",
   "metadata": {},
   "source": [
    "### Define the Model\n",
    "\n",
    "We use the `SimpleCNN3` model from the original notebook, which has three convolutional layers with batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c12e7-1df4-43b2-85a1-353e94c2d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-layer CNN with Batch Normalization\n",
    "class SimpleCNN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SimpleCNN3()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Prepare model and optimizer for distributed training\n",
    "model, optimizer = accelerator.prepare(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670cef3",
   "metadata": {},
   "source": [
    "### Training Loop with Distributed Training\n",
    "\n",
    "We implement the training loop with Accelerate, computing average loss and accuracy across GPUs using `gather_for_metrics` and logging metrics to WandB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0291729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 10\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather loss and predictions for metrics\n",
    "        loss = accelerator.gather_for_metrics(loss).mean().item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted = accelerator.gather_for_metrics(predicted)\n",
    "        labels = accelerator.gather_for_metrics(labels)\n",
    "\n",
    "        running_loss += loss * inputs.size(0)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_acc)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Gather loss and predictions for metrics\n",
    "            loss = accelerator.gather_for_metrics(loss).mean().item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predicted = accelerator.gather_for_metrics(predicted)\n",
    "            labels = accelerator.gather_for_metrics(labels)\n",
    "\n",
    "            val_loss += loss * inputs.size(0)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss / total\n",
    "    val_acc = 100 * correct / total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    # Log metrics to WandB (only on main process)\n",
    "    if accelerator.is_main_process:\n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': epoch_loss,\n",
    "            'train_accuracy': epoch_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_acc\n",
    "        })\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}% | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "# Finish WandB run\n",
    "if accelerator.is_main_process:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ac1b9e",
   "metadata": {},
   "source": [
    "### Visualize Training Curves\n",
    "\n",
    "We plot the training and validation loss/accuracy curves locally (in addition to WandB logging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c5b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss/accuracy (only on main process)\n",
    "if accelerator.is_main_process:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, epochs + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, epochs + 1), train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c5b6eb",
   "metadata": {},
   "source": [
    "### Running on Delta with SLURM\n",
    "\n",
    "To run this notebook on Delta with SLURM, create a job script (`run_job.sh`) with the following content:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=cifar10_distributed\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks-per-node=2\n",
    "#SBATCH --gpus-per-node=2\n",
    "#SBATCH --time=01:00:00\n",
    "#SBATCH --partition=gpu\n",
    "#SBATCH --account=<your_account>\n",
    "\n",
    "module load python\n",
    "module load cuda\n",
    "source activate myenv-CI-Pathway\n",
    "\n",
    "# Set WandB API key\n",
    "export WANDB_API_KEY=<your_wandb_api_key>\n",
    "\n",
    "# Run the notebook using accelerate\n",
    "accelerate launch --config_file accelerate_config.yaml session2_notebook_CNN_distributed.py\n",
    "```\n",
    "\n",
    "Create an Accelerate configuration file (`accelerate_config.yaml`):\n",
    "\n",
    "```yaml\n",
    "compute_environment: LOCAL_MACHINE\n",
    "distributed_type: MULTI_GPU\n",
    "num_processes: 2\n",
    "machine_rank: 0\n",
    "main_process_ip: null\n",
    "main_process_port: null\n",
    "```\n",
    "\n",
    "Convert the notebook to a Python script (`session2_notebook_CNN_distributed.py`) using:\n",
    "\n",
    "```bash\n",
    "jupyter nbconvert --to script session2_notebook_CNN_distributed.ipynb\n",
    "```\n",
    "\n",
    "Submit the job:\n",
    "\n",
    "```bash\n",
    "sbatch run_job.sh\n",
    "```\n",
    "\n",
    "### WandB Results\n",
    "\n",
    "The training curves (train/val loss and accuracy) are logged to WandB under the project `cifar10-distributed-training`. Visit your WandB dashboard to view the plots.\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "- **Distributed Training**: Using Accelerate, the model trains efficiently across 2 GPUs, with `gather_for_metrics` ensuring accurate metric aggregation.\n",
    "- **WandB Logging**: Training curves are logged for easy visualization and analysis.\n",
    "- **Model Performance**: The `SimpleCNN3` model achieves reasonable accuracy on CIFAR-10, with potential for improvement via hyperparameter tuning or deeper architectures.\n",
    "- **Depth and Channels**: As noted previously, deeper networks and more channels increase capacity but risk overfitting. Distributed training helps manage the computational load."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv-CI-Pathway",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
