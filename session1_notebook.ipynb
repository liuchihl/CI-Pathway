{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c699717b-0e06-4424-ba34-9daa1fa3a41d",
   "metadata": {},
   "source": [
    "### Train a Simple Classifier\n",
    "\n",
    "In our first session we learned about training a classifier on the MNIST Images. The thing about MNIST though is the dataset is very simple! Today we will repeat this, but with a slightly more complex dataset known as CIFAR10. This will also motivate why we love Convolutions later!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f29007a-db78-4200-8fe5-2d9e53e561d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04642c9f-093c-4131-8c7b-3c98d76b36d1",
   "metadata": {},
   "source": [
    "### Lets Explore the Data\n",
    "\n",
    "We will load the data here, lets take a quick look at what it looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036a7069-c5ca-4056-b676-dd95486fb77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "\n",
    "sample, label = next(iter(trainset))\n",
    "\n",
    "plt.imshow(np.array(sample))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aedd0d-ad5b-4ca2-af8e-388e9ba1c757",
   "metadata": {},
   "source": [
    "Thats a Frog! Just like MNIST there are 10 classes of images that we need to classify between!\n",
    "\n",
    "### Define a Model\n",
    "\n",
    "You can use any model shape you want, just use Linear layers only though. This means you can test how many hidden nodes to have in intermediate layers, and also how many layers you want in total. You can also play with different activation functions to see what works better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c12e7-1df4-43b2-85a1-353e94c2d31a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m         x = \u001b[38;5;28mself\u001b[39m.fc3(x)\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mmodel\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "class LinearNN(nn.Module):\n",
    "    def __init__(self, input_size=32*32*3, num_classes=10):\n",
    "        super(LinearNN, self).__init__()\n",
    "        \n",
    "        # Define the model parts using linear layers\n",
    "        self.fc1 = nn.Linear(input_size, 512)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(512, 256)        # Second hidden layer\n",
    "        self.fc3 = nn.Linear(256, num_classes)  # Output layer\n",
    "        self.relu = nn.ReLU()  # Activation function\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten the input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Pass through the layers with activation\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "model = LinearNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceed55dc-b985-4018-b2c4-ad5647573331",
   "metadata": {},
   "source": [
    "### Define a Dataloader\n",
    "\n",
    "We Now need to prepare our dataloader with the batch size we want to train with! I provide the datasets here (with some simple image transforms such as converting PIL to tensor and then normalizing). Create the dataloaders from that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1872b9-2e0a-4f24-9a2f-ca0a8062bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "BATCH_SIZE = \n",
    "trainloader = \n",
    "testloader = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c67b72e-eb83-4bdf-aafe-022d2d6ac75d",
   "metadata": {},
   "source": [
    "### Train a Model\n",
    "\n",
    "Train your model for however many epochs you want. Make sure to calculate the average loss and accuracy per epoch for both the training set and evaluation set so we can plot our training curves after!\n",
    "\n",
    "Things we need to set:\n",
    "\n",
    "- Device\n",
    "- Loss Function (what loss do we use for classification?)\n",
    "- Optimizer (you can use whatever you want, and whatever learning rate you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb312da-ac5b-4d78-b2c5-3fbef4472a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the Device ###\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "### Set the Loss Function ###\n",
    "criterion = \n",
    "\n",
    "### Load the Model ###\n",
    "model = \n",
    "\n",
    "### Set the Optimizer ###\n",
    "learning_rate = \n",
    "optimizer = \n",
    "\n",
    "### Training Loop ###\n",
    "epochs = \n",
    "\n",
    "# Store metrics\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training and validation loop\n",
    "epochs = \n",
    "\n",
    "### WRITE TRAINING LOOP ###\n",
    "### STORE THE PER EPOCH TRAIN/VAL LOSS/ACC IN THE ## \n",
    "### STORE METRICS LIST ABOVE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3ba3b-eb34-4e39-ac36-4cf32527e81a",
   "metadata": {},
   "source": [
    "### Plot Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1628551-5db5-4f9d-baeb-edf44f18eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
